import xml.etree.ElementTree as ET
import json
import os
import urllib.request
from unidiff import PatchSet, PatchedFile
import pandas as pd
import hashlib
import copy
from parsing_diffs import parse_hunk


diff_dir = "raw_dataset/diffs"
analysis_dir = "raw_dataset/analysis_files"
refined_dataset_dir = "refined_dataset"
repositories_dir = "submodule_repos_to_analyze"


df_repos = pd.read_csv("github_repos.csv")
analysis_files = [f.name for f in os.scandir(
    analysis_dir) if f.is_file() and not f.name == ".DS_Store"]
refined_data_files = [f.name.split("-")[0] for f in os.scandir(
    refined_dataset_dir) if f.is_file()]

# Instantiate this multiple times later:
refined_data_sample = {
    "Repo": "",
    "RepoURL": "",
    "SolutionFile": "",
    "FilePath": "",
    "NumberFileLines": "",
    "Commit": "",
    "FileURL": "",
    "DiagnosticID": "",
    "AnalyzerNuGet": "",
    "Severity": "",
    "DiagnosticOccurances": [
        # {
        #     "Message": "",
        #     "Line": 0,
        #     "Character": 0
        # },
    ],
    "ParsedDiff": {
        "ReplacedLines": [
            # {
            #     "SourceLocations": [4, 5],
            #     "TargetLines": ["static int i = 0;"]
            # }
        ],
        "RemovedLines": [],
        "AddedLines": [
            # {
            #     "TargetStartLocation": 34,
            #     "TargetLines": ["static int i = 0;"]
            # }
        ]
    },
    "FileContent": [],
}


def hash_filename(filename):
    """Using hash function to avoid OS Errors, too  long filename"""
    return f"{str(int(hashlib.sha256(filename.encode('utf-8')).hexdigest(), 16) % 10**8)}"


def filter_analysis_files(repo_name, solution_filename, nuget_full_name):

    def filter_func(analysis_filename):
        """
        $ANALYSIS_FILEPATH = "${Using:REPO_NAME}__${SOLUTION_FILENAME}__${Using:LAST_COMMIT}__${NUGET_FULL_NAME}.xml"

        Sample ANALYSIS_FILEPATH:
        raw_dataset\analysis_files\runtime__Common.Tests.sln__dcf816579c616e1d172d587301a0a769aa1c0771__AkzenteIT.Analyzers.1.0.6304.37642.xml

        Disregarding LAST_COMMIT for now.
        """
        REPO_NAME, SOLUTION_FILENAME, LAST_COMMIT, NUGET_FULL_NAME = analysis_filename[:-len(
            '.xml')].split("__")
        if repo_name == REPO_NAME and solution_filename == SOLUTION_FILENAME and nuget_full_name == NUGET_FULL_NAME:
            return True
        else:
            return False

    return filter_func


def filter_diagnostic_occurance(new_occurance_dict):
    def filter_func(existing_occurance_dict):
        if (
            existing_occurance_dict["Message"] == new_occurance_dict["Message"] and
            existing_occurance_dict["Line"] == new_occurance_dict["Line"] and
            existing_occurance_dict["Character"] == new_occurance_dict["Character"]
        ):
            return True
        return False
    return filter_func


diff_files = [f.name for f in os.scandir(diff_dir) if f.is_file()]
for diff_file in diff_files:

    print("diff_file: ", diff_file)

    # $ANALYSIS_FILEPATH = "${Using:REPO_NAME}__${SOLUTION_FILENAME}__${Using:LAST_COMMIT}__${NUGET_FULL_NAME}__${DIAGNOSTIC_ID}.diff"
    REPO_NAME, SOLUTION_FILENAME, LAST_COMMIT, NUGET_FULL_NAME, DIAGNOSTIC_ID = diff_file[:-len(
        '.diff')].split("__")

    # Prepare XML analysis file
    analysis_file_for_diff = list(filter(filter_analysis_files(
        REPO_NAME, SOLUTION_FILENAME, NUGET_FULL_NAME), analysis_files))
    print("analysis_file_for_diff: ", analysis_file_for_diff)
    if len(analysis_file_for_diff) != 1:
        print("More than one analysis_file_for_diff!")
        # input("Press Enter to continue...")
    analysis_file_for_diff = analysis_file_for_diff[0]
    # Generated by Roslynator as 'Filepath'
    analyzed_file_prefix = f"C:\\Users\\vlohse\Desktop\\neural-repair-static-analysis\\submodule_repos_to_analyze\\{REPO_NAME}\\"

    tree = ET.parse(f"{analysis_dir}/{analysis_file_for_diff}")
    root = tree.getroot()  # <Roslynator></Roslynator> is root
    projects_analysed = root.find('CodeAnalysis').find('Projects')

    patch_set = PatchSet.from_filename(f"{diff_dir}/{diff_file}")

    repo_dir = f"{repositories_dir}/{REPO_NAME}"

    # One patch per file
    for patched_file in patch_set:
        if patched_file.is_added_file or patched_file.is_removed_file:
            continue

        print("patched_file.path: ", patched_file.path)

        parsed_file_path = patched_file.path.replace(
            "/", "--").replace("\\", "--")
        refined_data_filename = f"{diff_file[:-len('.diff')]}__{parsed_file_path}"
        refined_data_filename_hash = f"{hash_filename(refined_data_filename)}"
        if refined_data_filename_hash in refined_data_files:
            print(
                f"refined_data_filename_hash already exists! file: {refined_data_filename}")
            # Appending refined_data_filename_hash to refined_data_files later on
            continue

        refined_data_file = copy.deepcopy(refined_data_sample)

        repo_row = df_repos.loc[df_repos['RepoName'] == REPO_NAME].iloc[0]

        refined_data_file["Repo"] = REPO_NAME
        refined_data_file["RepoURL"] = repo_row["RepoURL"]
        refined_data_file["SolutionFile"] = SOLUTION_FILENAME
        refined_data_file["FilePath"] = patched_file.path
        refined_data_file["Commit"] = LAST_COMMIT
        refined_data_file["DiagnosticID"] = DIAGNOSTIC_ID
        refined_data_file["AnalyzerNuGet"] = NUGET_FULL_NAME

        repo_url = repo_row["RepoURL"]
        if "https://github.com" in repo_url:
            repo_url = repo_url[:-len('.git')] if repo_url.endswith('.git') else repo_url
            refined_data_file["FileURL"] = f"{repo_url}/blob/{LAST_COMMIT}/{patched_file.path}"

        # TODO: Do this later and truncate file
        # with open(f"{repo_dir}/{patched_file.path}") as f:
        #     # my_list = [x.rstrip() for x in f] # remove line breaks
        #     refined_data_file["FileContent"] = list(f)
        #     refined_data_file["NumberFileLines"] = len(
        #         list(refined_data_file["FileContent"]))

        all_replaced_lines = []
        all_added_lines = []
        all_removed_lines = []
        for hunk in patched_file:
            replaced_lines, added_lines, removed_lines = parse_hunk(hunk)
            all_replaced_lines += replaced_lines
            all_added_lines += added_lines
            all_removed_lines += removed_lines

        count = 0
        project_filepaths = []
        unique_diagnostic_occurances = []
        for xml_project in projects_analysed:

            # TODO: Check whether some analysis is based on target framework
            # Occasionally, Roslynator may perform the same analysis multiple times (due to multiple target frameworks in sln file, etc.)
            cs_proj_path = xml_project.get('FilePath')
            if cs_proj_path in project_filepaths:
                print(
                    f"This .csproj has been analysed multiple times! cs_proj_path: {cs_proj_path}")
                continue
            else:
                project_filepaths.append(cs_proj_path)

            for xml_diagnostic in xml_project.find('Diagnostics'):

                if not hasattr(xml_diagnostic.find('FilePath'), 'text'):
                    # Happens rarely. Example:
                    # CA9998; FxCopAnalyzers package has been deprecated in favor of 'Microsoft.CodeAnalysis.NetAnalyzers'

                    # Cannot be sure that this diagnostic led to fix in our file
                    continue

                analyzed_file_filepath = xml_diagnostic.find(
                    'FilePath').text[len(analyzed_file_prefix):].replace("\\", "/")

                if xml_diagnostic.get('Id') != DIAGNOSTIC_ID or analyzed_file_filepath != patched_file.path:
                    continue

                # Just do this once
                if count == 0:
                    refined_data_file["Severity"] = xml_diagnostic.find(
                        'Severity').text
                    count += 1

                new_occurance_dict = {
                    "Message": xml_diagnostic.find('Message').text,
                    "Line": int(xml_diagnostic.find('Location').get('Line')),
                    "Character": int(xml_diagnostic.find('Location').get('Character'))
                }

                # Even though already checking for .csproj duplicates earlier, one file may be referenced
                # in multiple different projects as well.
                # Example: SA1642 for <Location Line="55" Character="16" /> in analysis file
                # Druntime__Microsoft.Bcl.AsyncInterfaces.sln__e98d043d7d293c88a346b632d8fc12564a8ef0ce__Documentation.Analyser.1.1.1.xml
                occurance_duplicates = filter(filter_diagnostic_occurance(
                    new_occurance_dict), unique_diagnostic_occurances)
                if len(list(occurance_duplicates)) != 0:
                    print(
                        f"Duplicate DiagnosticOccurance! new_occurance_dict: {new_occurance_dict}")
                    # input("Press Enter to continue...")
                    continue

                unique_diagnostic_occurances.append(new_occurance_dict)

        """
        To which diff-batch does each diagnostic correspond to?
        In some cases, multiple diagnostic occurances may have generated
        one "diff batch". An example would be two occurances in the same line,
        but at different characters. If the line was deleted in the diff, we would
        not know which diagnostic occurance caused this to happen. Therefore,
        these occurances are bundled. 
        One diagnostic occurance can however only have generated a single 
        diff batch. The assumption is that this diff batch will be at the same
        line as the diagnostic occurance.
        """
        diff_batch_to_diagnostic_occurances_dict = {}
        for diagnostic_occurance in unique_diagnostic_occurances:

            diff_key = None
            for count, value in enumerate(all_replaced_lines):
                if diagnostic_occurance["Line"] in value["SourceLocations"]:
                    diff_key = f"REPLACE-{count}"
                    break

            for count, value in enumerate(all_added_lines):
                if diagnostic_occurance["Line"] == value["PreviousSourceLocation"]:
                    diff_key = f"ADD-{count}"
                    break

            for count, value in enumerate(all_removed_lines):
                if (diagnostic_occurance["Line"] >= value["SourceLocationStart"] and
                        diagnostic_occurance["Line"] <= value["SourceLocationEnd"]):
                    diff_key = f"REMOVE-{count}"
                    break
            
            # Diagnostic occurance leads to no obvious diff batch
            if not diff_key:
                continue

            if diff_key not in diff_batch_to_diagnostic_occurances_dict:
                diff_batch_to_diagnostic_occurances_dict[diff_key] = []
            diff_batch_to_diagnostic_occurances_dict[diff_key].append(
                diagnostic_occurance)

        num_diff_datapoint = 0
        # Creating one datapoint per diff action (add/delete/replace)
        for key, value in diff_batch_to_diagnostic_occurances_dict.items():

            refined_data = copy.deepcopy(refined_data_file)
            refined_data["DiagnosticOccurances"] = value

            diff_action, action_num = key.split("-")
            refined_data["ParsedDiff"] = {}
            refined_data["ParsedDiff"]["ActionType"] = diff_action
            if diff_action == "REPLACE":
                refined_data["ParsedDiff"]["Action"] = all_replaced_lines[int(action_num)]
            elif diff_action == "ADD":
                refined_data["ParsedDiff"]["Action"] = all_added_lines[int(action_num)]
            elif diff_action == "REMOVE":
                refined_data["ParsedDiff"]["Action"] = all_removed_lines[int(action_num)]

            with open(f"{refined_dataset_dir}/{refined_data_filename_hash}-{num_diff_datapoint}.json", 'w', encoding='utf-8') as f:
                json.dump(refined_data, f, ensure_ascii=False, indent=2)
            print("Created refined_data_filename: ", refined_data_filename)
            refined_data_files.append(refined_data_filename_hash)
            num_diff_datapoint += 1

            # TODO: Concatenate file and adjust indices
